{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c26119c",
   "metadata": {},
   "source": [
    "# Wrangle Exercises\n",
    "Data Acquisition\n",
    "These exercises should go in a notebook or script named ```wrangle```. Add, commit, and push your changes.\n",
    "\n",
    "This exercise uses the ```cases```, ```dept```, and ```source``` tables from the ```311_data``` on the ```Codeup MySQL server```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9dd47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/22 12:49:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/01/22 12:49:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/01/22 12:49:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "\n",
    "# The SparkSession is where you would specify the JDBC driver and additional connection details.\n",
    "# We'll use pd.read_sql to simplify here so we can focus on the Spark API and not the IT setup.\n",
    "# When using Spark on the job, you'll work with the operations team to install the right Java drivers and configure your connection\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ------------- #\n",
    "# Local Imports #\n",
    "# ------------- #\n",
    "\n",
    "# importing sys\n",
    "import sys\n",
    "\n",
    "# adding 00_helper_files to the system path\n",
    "sys.path.insert(0, '/Users/qmcbt/codeup-data-science/00_helper_files')\n",
    "\n",
    "# env containing sensitive access credentials\n",
    "import env\n",
    "from env import user, password, host\n",
    "from env import get_db_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3741c",
   "metadata": {},
   "source": [
    "## 1. Read the ```cases```, ```dept```, and ```source data``` into their own spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81504faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------------+\n",
      "|index|source_id| source_username|\n",
      "+-----+---------+----------------+\n",
      "|    0|   100137|Merlene Blodgett|\n",
      "|    1|   103582|     Carmen Cura|\n",
      "|    2|   106463| Richard Sanchez|\n",
      "|    3|   119403|  Betty De Hoyos|\n",
      "|    4|   119555|  Socorro Quiara|\n",
      "+-----+---------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in source to a spark dataframe\n",
    "query = \"\"\"SELECT * FROM source\"\"\"\n",
    "url = get_db_url(\"311_data\")\n",
    "source_df = pd.read_sql(query, url)\n",
    "source_df = spark.createDataFrame(source_df)\n",
    "source_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4928fbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/22 13:00:59 WARN TaskSetManager: Stage 3 contains a task of very large size (18865 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/22 13:01:04 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 3): Attempting to kill Python Worker\n",
      "+----------+----------------+----------------+------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|   case_id|case_opened_date|case_closed_date|SLA_due_date|case_late|num_days_late|case_closed|   dept_division|service_request_type|   SLA_days|case_status|source_id|     request_address|council_district|\n",
      "+----------+----------------+----------------+------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|1014127332|     1/1/18 0:42|    1/1/18 12:29|9/26/20 0:42|       NO| -998.5087616|        YES|Field Operations|        Stray Animal|      999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|               5|\n",
      "|1014127333|     1/1/18 0:46|     1/3/18 8:11| 1/5/18 8:30|       NO| -2.012604167|        YES|     Storm Water|Removal Of Obstru...|4.322222222|     Closed| svcCRMSS|2215  GOLIAD RD, ...|               3|\n",
      "|1014127334|     1/1/18 0:48|     1/2/18 7:57| 1/5/18 8:30|       NO| -3.022337963|        YES|     Storm Water|Removal Of Obstru...|4.320729167|     Closed| svcCRMSS|102  PALFREY ST W...|               3|\n",
      "|1014127335|     1/1/18 1:29|     1/2/18 8:13|1/17/18 8:30|       NO| -15.01148148|        YES|Code Enforcement|Front Or Side Yar...|16.29188657|     Closed| svcCRMSS|114  LA GARDE ST,...|               3|\n",
      "|1014127336|     1/1/18 1:34|    1/1/18 13:29| 1/1/18 4:34|      YES|  0.372164352|        YES|Field Operations|Animal Cruelty(Cr...|      0.125|     Closed| svcCRMSS|734  CLEARVIEW DR...|               7|\n",
      "+----------+----------------+----------------+------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read in cases to a spark dataframe\n",
    "query = \"\"\"SELECT * FROM cases\"\"\"\n",
    "url = get_db_url(\"311_data\")\n",
    "cases_df = pd.read_sql(query, url)\n",
    "cases_df = spark.createDataFrame(cases_df)\n",
    "cases_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f0a6c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|       dept_division|           dept_name|standardized_dept_name|dept_subject_to_SLA|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|     311 Call Center|    Customer Service|      Customer Service|                YES|\n",
      "|               Brush|Solid Waste Manag...|           Solid Waste|                YES|\n",
      "|     Clean and Green|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|Clean and Green N...|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|    Code Enforcement|Code Enforcement ...|  DSD/Code Enforcement|                YES|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in source to a spark dataframe\n",
    "query = \"\"\"SELECT * FROM dept\"\"\"\n",
    "url = get_db_url(\"311_data\")\n",
    "dept_df = pd.read_sql(query, url)\n",
    "dept_df = spark.createDataFrame(dept_df)\n",
    "dept_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e57271",
   "metadata": {},
   "source": [
    "## 2. Let's see how writing to the local disk works in spark:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e8478",
   "metadata": {},
   "source": [
    "* Write the code necessary to store the source data in both csv and json format, store these as ```sources_csv``` and ```sources_json```  \n",
    "### ANSWER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e428da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write the .json file in a data folder overwriting any existing file with the same name\n",
    "source_df.write.json(\"data/source_json\", mode=\"overwrite\")\n",
    "\n",
    "# Write the .csv file in a data folder overwriting any existing file with the same name\n",
    "(\n",
    "    source_df.write.format(\"csv\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(\"data/source_csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17d0676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m.\u001b[m\u001b[m                         \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m        \u001b[34mdata\u001b[m\u001b[m\r\n",
      "\u001b[34m..\u001b[m\u001b[m                        NOTES_spark.ipynb         spark-wrangle.ipynb\r\n",
      "\u001b[34m.git\u001b[m\u001b[m                      NOTES_spark_wrangle.ipynb spark101.ipynb\r\n",
      ".gitignore                README.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caa85eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mmpg_csv\u001b[m\u001b[m     \u001b[34mmpg_json\u001b[m\u001b[m    \u001b[34msource_csv\u001b[m\u001b[m  \u001b[34msource_json\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b751adf0",
   "metadata": {},
   "source": [
    "* Inspect your folder structure. What do you notice?  \n",
    "### ANSWER: There is now a folder named data that was created when we initiated spark to createDataFrame; both the .json and .csv files are saved in that folder because we chose that directory in our code above with ```data/```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54262ac3",
   "metadata": {},
   "source": [
    "## 3. Inspect the data in your dataframes. Are the data types appropriate? Write the code necessary to cast the values to the appropriate types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "805eb527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('index', LongType(), True), StructField('source_id', StringType(), True), StructField('source_username', StringType(), True)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The .schema attribute shows the data types that Spark has inferred from the source\n",
    "source_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e07d1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('case_id', LongType(), True), StructField('case_opened_date', StringType(), True), StructField('case_closed_date', StringType(), True), StructField('SLA_due_date', StringType(), True), StructField('case_late', StringType(), True), StructField('num_days_late', DoubleType(), True), StructField('case_closed', StringType(), True), StructField('dept_division', StringType(), True), StructField('service_request_type', StringType(), True), StructField('SLA_days', DoubleType(), True), StructField('case_status', StringType(), True), StructField('source_id', StringType(), True), StructField('request_address', StringType(), True), StructField('council_district', LongType(), True)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The .schema attribute shows the data types that Spark has inferred from the source\n",
    "cases_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcd1c91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dept_division', StringType(), True), StructField('dept_name', StringType(), True), StructField('standardized_dept_name', StringType(), True), StructField('dept_subject_to_SLA', StringType(), True)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The .schema attribute shows the data types that Spark has inferred from the source\n",
    "dept_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481e7cd",
   "metadata": {},
   "source": [
    "### 1. How old is the latest (in terms of days past SLA) currently open issue? How long has the oldest (in terms of days since opened) currently opened issue been open?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47af9d1",
   "metadata": {},
   "source": [
    "### 2. How many Stray Animal cases are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b332081",
   "metadata": {},
   "source": [
    "### 3. How many service requests that are assigned to the Field Operations department (```dept_division```) are not classified as \"Officer Standby\" request type (```service_request_type```)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96727186",
   "metadata": {},
   "source": [
    "### 4. Convert the ```council_district``` column to a ```string``` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4602fea",
   "metadata": {},
   "source": [
    "### 5. Extract the year from the ```case_closed_date``` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25fb48",
   "metadata": {},
   "source": [
    "### 6. Convert ```num_days_late``` from days to hours in new columns ```num_hours_late```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f1719",
   "metadata": {},
   "source": [
    "### 7. Join the case data with the source and department data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eeef98",
   "metadata": {},
   "source": [
    "### 8. Are there any cases that do not have a request source?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ec7e1",
   "metadata": {},
   "source": [
    "### 9. What are the top 10 service request types in terms of number of requests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a505449",
   "metadata": {},
   "source": [
    "### 10. What are the top 10 service request types in terms of average days late?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48934771",
   "metadata": {},
   "source": [
    "### 11. Does number of days late depend on department?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e2a66",
   "metadata": {},
   "source": [
    "### 12. How do number of days late depend on department and request type?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb286ec",
   "metadata": {},
   "source": [
    "## You might have noticed that the latest date in the dataset is fairly far off from the present day. To account for this, replace any occurances of the current time with the maximum date from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98110d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
